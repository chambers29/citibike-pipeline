{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a190f2-9442-4ccd-bcfb-004a6636014c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#FUNCTIONS TO NORMALIZE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4e598f-8f44-456a-8fb7-2db3ce4f1667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lower, lit, to_timestamp, try_to_timestamp, coalesce, year, month, expr, format_string, min as fmin, max as fmax\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "##### CHECK IF FORMAT IS NEW #####\n",
    "def new_format(df):\n",
    "    return \"member_casual\" in df.columns\n",
    "\n",
    "##### NORMALIZE OLD FORMAT #####\n",
    "def normalize_old(df, year_dir):\n",
    "\n",
    "    ALIASES = {\n",
    "        \"starttime\": \"started_at\",\n",
    "        \"stoptime\": \"ended_at\",\n",
    "        \"start station name\": \"start_station_name\",\n",
    "        \"start station id\": \"start_station_id\",\n",
    "        \"end station name\": \"end_station_name\",\n",
    "        \"end station id\": \"end_station_id\",\n",
    "        \"start station latitude\": \"start_lat\",\n",
    "        \"start station longitude\": \"start_lng\",\n",
    "        \"end station latitude\": \"end_lat\",\n",
    "        \"end station longitude\": \"end_lng\",\n",
    "        \"usertype\": \"member_casual\"\n",
    "    }\n",
    "\n",
    "    VALUE_MAP = {\n",
    "        \"subscriber\": \"member\",\n",
    "        \"customer\": \"casual\"\n",
    "    }\n",
    "\n",
    "    # MAPPING COLUMN NAMES\n",
    "    for old_column, new_column in ALIASES.items():\n",
    "        if old_column in df.columns:\n",
    "            df = df.withColumnRenamed(old_column, new_column)\n",
    "        else: print(f\"MAPPING COLUMN NAMES ERROR: Couldn't find column: {old_column}\")\n",
    "    print(\"column mapping finished\")\n",
    "    \n",
    "    # MAPPING VALUES MEMBER_CASUAL\n",
    "    if \"member_casual\" in df.columns:\n",
    "        for old_value, new_value in VALUE_MAP.items():\n",
    "            df = df.withColumn(\n",
    "                \"member_casual\",\n",
    "                when(lower(col(\"member_casual\")) == old_value, new_value)\n",
    "                .otherwise(col(\"member_casual\"))\n",
    "            )\n",
    "        print(\"member_casual mapping finished\")\n",
    "    else: print(f\"MAPPING VALUES MEMBER_CASUAL ERROR: Couldn't find column: member_casual\")\n",
    "\n",
    "    print(\"(OLD) Standardizing column names and values...\")\n",
    "    df = (\n",
    "        date_parser(df)\n",
    "        .withColumn(\"ride_id\", lit(None))\n",
    "        .withColumn(\"rideable_type\", lit(None))\n",
    "        .withColumn(\"duration_sec\", (col(\"ended_at\").cast(\"long\") - col(\"started_at\").cast(\"long\")))\n",
    "        .withColumn(\"year\", year(col(\"ended_at\")))\n",
    "        .withColumn(\"month\", format_string(\"%02d\", month(col(\"ended_at\"))))\n",
    "        .select(\n",
    "            \"ride_id\",\n",
    "            \"rideable_type\",\n",
    "            \"duration_sec\",\n",
    "            \"started_at\",\n",
    "            \"ended_at\",\n",
    "            \"start_station_name\",\n",
    "            \"start_station_id\",\n",
    "            \"end_station_name\",\n",
    "            \"end_station_id\",\n",
    "            \"start_lat\",\n",
    "            \"start_lng\",\n",
    "            \"end_lat\",\n",
    "            \"end_lng\",\n",
    "            \"member_casual\",\n",
    "            \"year\",\n",
    "            \"month\"\n",
    "        )\n",
    "    )\n",
    "    print(\"Standardizing column names and values finished\")\n",
    "    return df\n",
    "\n",
    "##### NORMALIZE NEW FORMAT #####\n",
    "def normalize_new(df, year_dir):\n",
    "    print(\"(NEW) Standardizing column names and values...\")\n",
    "    df = (\n",
    "        date_parser(df)\n",
    "        .withColumn(\"duration_sec\", (col(\"ended_at\").cast(\"long\") - col(\"started_at\").cast(\"long\")))\n",
    "        .withColumn(\"year\", year(col(\"ended_at\")))\n",
    "        .withColumn(\"month\", format_string(\"%02d\", month(col(\"ended_at\"))))\n",
    "        .select(\n",
    "            \"ride_id\",\n",
    "            \"rideable_type\",\n",
    "            \"duration_sec\",\n",
    "            \"started_at\",\n",
    "            \"ended_at\",\n",
    "            \"start_station_name\",\n",
    "            \"start_station_id\",\n",
    "            \"end_station_name\",\n",
    "            \"end_station_id\",\n",
    "            \"start_lat\",\n",
    "            \"start_lng\",\n",
    "            \"end_lat\",\n",
    "            \"end_lng\",\n",
    "            \"member_casual\",\n",
    "            \"year\",\n",
    "            \"month\"\n",
    "        )\n",
    "    )\n",
    "    print(\"Standardizing column names and values finished\")\n",
    "    return df\n",
    "\n",
    "##### CONVERT TO SILVER FORMAT #####\n",
    "def to_silver(df):\n",
    "    print(\"Converting to silver format...\")\n",
    "    SILVER_SCHEMA = StructType([\n",
    "        StructField(\"ride_id\",              StringType(),       True),\n",
    "        StructField(\"rideable_type\",        StringType(),       True),\n",
    "        StructField(\"duration_sec\",         IntegerType(),      True),\n",
    "        StructField(\"started_at\",           TimestampType(),    True), # starttime\n",
    "        StructField(\"ended_at\",             TimestampType(),    True), # stoptime\n",
    "        StructField(\"start_station_name\",   StringType(),       True), # start station name\n",
    "        StructField(\"start_station_id\",     StringType(),       True), # start station id\n",
    "        StructField(\"end_station_name\",     StringType(),       True), # end station name\n",
    "        StructField(\"end_station_id\",       StringType(),       True), # end station id\n",
    "        StructField(\"start_lat\",            DoubleType(),       True), # start station latitude\n",
    "        StructField(\"start_lng\",            DoubleType(),       True), # start station longitude\n",
    "        StructField(\"end_lat\",              DoubleType(),       True), # end station latitude\n",
    "        StructField(\"end_lng\",              DoubleType(),       True), # end station longitude\n",
    "        StructField(\"member_casual\",        StringType(),       True), # usertype (values: subscriber, customer)\n",
    "        StructField(\"year\",                 IntegerType(),      True)\n",
    "        #StructField(\"bikeid\", IntegerType(), True), -bikeid\n",
    "        #StructField(\"birth year\", IntegerType(), True), -birthyear\n",
    "        #StructField(\"gender\", IntegerType(), True) -gender\n",
    "    ])\n",
    "\n",
    "    silver_cols = [f.name for f in SILVER_SCHEMA.fields]\n",
    "    silver_types = {f.name: f.dataType for f in SILVER_SCHEMA.fields}\n",
    "    print(\"Silver format conversion complete.\")\n",
    "    return df.select(\n",
    "        [col(c).cast(silver_types[c]) for c in silver_cols]\n",
    "    )\n",
    "\n",
    "def date_parser(df):\n",
    "    print(\"Parsing date...\")\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\n",
    "            \"started_at\",\n",
    "            coalesce(\n",
    "                try_to_timestamp(col(\"started_at\"), lit(\"yyyy-MM-dd HH:mm:ss.SSSS\")),\n",
    "                try_to_timestamp(col(\"started_at\"), lit(\"yyyy-MM-dd HH:mm:ss.SSS\")),\n",
    "                try_to_timestamp(col(\"started_at\"), lit(\"yyyy-MM-dd HH:mm:ss\")),\n",
    "                try_to_timestamp(col(\"started_at\"), lit(\"M/d/yyyy HH:mm:ss\")),\n",
    "                try_to_timestamp(col(\"started_at\"), lit(\"M/d/yyyy H:mm:ss\")),\n",
    "                try_to_timestamp(col(\"started_at\"), lit(\"M/d/yyyy HH:mm\")),\n",
    "                try_to_timestamp(col(\"started_at\"), lit(\"M/d/yyyy H:mm\")) \n",
    "            )\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"ended_at\",\n",
    "            coalesce(\n",
    "                try_to_timestamp(col(\"ended_at\"), lit(\"yyyy-MM-dd HH:mm:ss.SSSS\")),\n",
    "                try_to_timestamp(col(\"ended_at\"), lit(\"yyyy-MM-dd HH:mm:ss.SSS\")),\n",
    "                try_to_timestamp(col(\"ended_at\"), lit(\"yyyy-MM-dd HH:mm:ss\")),\n",
    "                try_to_timestamp(col(\"ended_at\"), lit(\"M/d/yyyy HH:mm:ss\")),\n",
    "                try_to_timestamp(col(\"ended_at\"), lit(\"M/d/yyyy H:mm:ss\")),\n",
    "                try_to_timestamp(col(\"ended_at\"), lit(\"M/d/yyyy HH:mm\")),\n",
    "                try_to_timestamp(col(\"ended_at\"), lit(\"M/d/yyyy H:mm\")) \n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # bad_rows = df.where(\n",
    "    #     (col(\"started_at\").isNull() | col(\"ended_at\").isNull())\n",
    "    # ).count()\n",
    "    # if bad_rows > 0:\n",
    "    #     display(\n",
    "    #         df.where(\n",
    "    #             (col(\"started_at\").isNull() | col(\"ended_at\").isNull())\n",
    "    #         )\n",
    "    #     )\n",
    "    #     raise Exception(f\"Parser error: {bad_rows} bad records\")\n",
    "\n",
    "    ymin, ymax = df.select(\n",
    "        fmin(year(col(\"ended_at\"))),\n",
    "        fmax(year(col(\"ended_at\")))\n",
    "    ).first()\n",
    "\n",
    "    print(f\"Parsing complete. Year range: {ymin} â€“ {ymax}\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc1ab64-c2eb-4a64-9ad7-636e66786acb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72460e1a-c946-4e5f-988e-8ce09825090e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ADLS_BRONZE = \"abfss://citibike@databricksjm.dfs.core.windows.net/bronze\"\n",
    "ADLS_SILVER = \"abfss://citibike@databricksjm.dfs.core.windows.net/silver\"\n",
    "ADLS_TEST = \"abfss://citibike@databricksjm.dfs.core.windows.net/test\"\n",
    "\n",
    "years = [fi.name.rstrip(\"/\") for fi in dbutils.fs.ls(ADLS_BRONZE) if fi.isDir()]\n",
    "#years = [\"2015\"]\n",
    "\n",
    "# Load every year separately\n",
    "for year_dir in years:\n",
    "    print(f\"********** PROCESSING: {year_dir} **********\")\n",
    "    whole_year_data = f\"{ADLS_BRONZE}/{year_dir}\"\n",
    "    df = (\n",
    "        spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", True)\n",
    "        .load(whole_year_data)\n",
    "    )\n",
    "    # SAMPLE TEST - COMMENT LATER\n",
    "    # df = df.sample(False, 0.02, seed=42).limit(200_000)\n",
    "\n",
    "    # Check df format and normalize it by proper function\n",
    "    if new_format(df):\n",
    "        df = normalize_new(df, year_dir)\n",
    "    else:\n",
    "        df = normalize_old(df, year_dir)\n",
    "\n",
    "    df = to_silver(df)\n",
    "    \n",
    "    print(\"Saving to silver...\")\n",
    "    (df.write\n",
    "     .format(\"delta\")\n",
    "     .mode(\"replaceWhere\", f\"year ={int(year_dir)}\")\n",
    "     .partitionBy(\"year\")\n",
    "     .save(ADLS_SILVER)\n",
    "     )\n",
    "    print(f\"********** FINISHED: {year_dir} **********\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14ebd54f-25a4-4259-91a8-66822ddf66f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
